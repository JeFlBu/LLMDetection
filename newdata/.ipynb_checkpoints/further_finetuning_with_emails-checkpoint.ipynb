{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90b4f77-dacd-45ea-80da-3111f76b4b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import json \n",
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "from generated_text_detector.utils.model.roberta_classifier import RobertaClassifier\n",
    "from transformers import AutoTokenizer, AdamW\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9173f354-c601-48e0-a283-75617868b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f=open('list_gen_gpt_400.json')\n",
    "tmp=json.load(f)\n",
    "f.close()\n",
    "obj=[]\n",
    "for elt in tmp:\n",
    "    obj+=[elt.lower().replace(',','').replace('.','').replace('?','').replace(';','').replace('!','').replace('\\n','').replace('\\'','')]\n",
    "del tmp\n",
    "obj={'rewrite':obj[:400],'merge':obj[400:800],'generate':obj[800:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d69f79-1fbb-4f97-9c3e-96e993e3968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a parquet file into a pandas DataFrame\n",
    "df_before = pd.read_parquet('before_gpt.parquet')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "# print(df_before.head())\n",
    "\n",
    "txt_emails_to_use=list(df_before['cleaned_text'][:1200])\n",
    "\n",
    "txt_emails_to_test=list(df_before['cleaned_text'][1200:1400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521d2b0d-1e24-41d6-94d2-7a246b5e0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items = list(obj.keys())\n",
    "\n",
    "# Get all combinations of different lengths\n",
    "all_combinations = []\n",
    "for r in range(1, len(items) + 1):\n",
    "    all_combinations.extend(combinations(items, r))\n",
    "\n",
    "to_implement=[[]]\n",
    "for combo in all_combinations:\n",
    "    to_implement+=[list(combo)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05efe1a-b0f9-46bb-bc04-7933b8d4af30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['rewrite'],\n",
       " ['merge'],\n",
       " ['generate'],\n",
       " ['rewrite', 'merge'],\n",
       " ['rewrite', 'generate'],\n",
       " ['merge', 'generate'],\n",
       " ['rewrite', 'merge', 'generate']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a977e03-bb96-430e-a0ad-3986375fdb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4613042c-58d6-4d80-ae23-e56ce6e6beea",
   "metadata": {},
   "source": [
    "for strategy in to_implement:\n",
    "    name_folder='strategy'+'_'.join(strategy)\n",
    "    save_directory = name_folder+\"/fine_tuned_roberta_model_1e-7\"\n",
    "    if os.path.exists(save_directory) or len(strategy)==0:\n",
    "        print('model already trained')\n",
    "        continue\n",
    "    if not os.path.exists(name_folder):\n",
    "        os.mkdir(name_folder)\n",
    "    train_texts=[]\n",
    "    train_labels=[]\n",
    "    for key in strategy:\n",
    "        train_texts+=obj[key]\n",
    "        train_labels+=[1 for x in obj[key]]\n",
    "    train_texts+=txt_emails_to_use[:len(train_texts)]\n",
    "    train_labels+=[0 for x in train_labels]\n",
    "\n",
    "    # Initialize the model and tokenizer\n",
    "    model = RobertaClassifier.from_pretrained(\"SuperAnnotate/roberta-large-llm-content-detector\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"SuperAnnotate/roberta-large-llm-content-detector\")\n",
    "\n",
    "        \n",
    "    # Data preparation\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Fine-tuning setup\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-7)\n",
    "    model.train()\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)[1]  # Index 1 if logits are the second item in the tuple\n",
    "\n",
    "            # Rest of the code remains the same\n",
    "            loss = F.binary_cross_entropy_with_logits(logits.squeeze(-1), labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "    \n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ac6f444-7cb2-4885-8bd4-e458ca933408",
   "metadata": {},
   "source": [
    "####WITH 2E-5\n",
    "\n",
    "model already trained\n",
    "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
    "  warnings.warn(\n",
    "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "  warnings.warn(\n",
    "Epoch 1/3, Loss: 0.7299395994842053\n",
    "Epoch 2/3, Loss: 0.7010093075037003\n",
    "Epoch 3/3, Loss: 0.7091625761985779\n",
    "Model and tokenizer saved to strategyrewrite/fine_tuned_roberta_model\n",
    "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
    "  warnings.warn(\n",
    "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "  warnings.warn(\n",
    "Epoch 1/3, Loss: 0.7059680958092213\n",
    "Epoch 2/3, Loss: 0.7024015192687512\n",
    "Epoch 3/3, Loss: 0.7041943448781968\n",
    "Model and tokenizer saved to strategymerge/fine_tuned_roberta_model\n",
    "Epoch 1/3, Loss: 0.05254493124622968\n",
    "Epoch 2/3, Loss: 0.03300542788376333\n",
    "Epoch 3/3, Loss: 0.028989977332676064\n",
    "Model and tokenizer saved to strategygenerate/fine_tuned_roberta_model\n",
    "Epoch 1/3, Loss: 0.7195342077314854\n",
    "Epoch 2/3, Loss: 0.7068300146143883\n",
    "Epoch 3/3, Loss: 0.7155956146121025\n",
    "Model and tokenizer saved to strategyrewrite_merge/fine_tuned_roberta_model\n",
    "Epoch 1/3, Loss: 0.7182651668973267\n",
    "Epoch 2/3, Loss: 0.709076013341546\n",
    "Epoch 3/3, Loss: 0.7011783367395401\n",
    "Model and tokenizer saved to strategyrewrite_generate/fine_tuned_roberta_model\n",
    "Epoch 1/3, Loss: 0.4934926855468075\n",
    "Epoch 2/3, Loss: 0.7184478555619717\n",
    "Epoch 3/3, Loss: 0.6811328674385732\n",
    "Model and tokenizer saved to strategymerge_generate/fine_tuned_roberta_model\n",
    "Epoch 1/3, Loss: 0.25583457760105377\n",
    "Epoch 2/3, Loss: 0.21710065735668954\n",
    "Epoch 3/3, Loss: 0.20611246337968622\n",
    "Model and tokenizer saved to strategyrewrite_merge_generate/fine_tuned_roberta_model\n",
    "\n",
    "\n",
    "\n",
    "### with 1e-7\n",
    "\n",
    "model already trained\n",
    "Epoch 1/3, Loss: 0.9876947166025638\n",
    "Epoch 2/3, Loss: 0.37388330068439246\n",
    "Epoch 3/3, Loss: 0.25371829636394977\n",
    "Model and tokenizer saved to strategyrewrite/fine_tuned_roberta_model_1e-7\n",
    "Epoch 1/3, Loss: 0.9538972620666027\n",
    "Epoch 2/3, Loss: 0.39956533901393415\n",
    "Epoch 3/3, Loss: 0.2770251676440239\n",
    "Model and tokenizer saved to strategymerge/fine_tuned_roberta_model_1e-7\n",
    "Epoch 1/3, Loss: 0.622638253569603\n",
    "Epoch 2/3, Loss: 0.11598200395703316\n",
    "Epoch 3/3, Loss: 0.05514413109980523\n",
    "Model and tokenizer saved to strategygenerate/fine_tuned_roberta_model_1e-7\n",
    "Epoch 1/3, Loss: 0.6598842566087842\n",
    "Epoch 2/3, Loss: 0.24254981056787073\n",
    "Epoch 3/3, Loss: 0.18722050079610197\n",
    "Model and tokenizer saved to strategyrewrite_merge/fine_tuned_roberta_model_1e-7\n",
    "Epoch 1/3, Loss: 0.5749684432893991\n",
    "Epoch 2/3, Loss: 0.16417738294694573\n",
    "Epoch 3/3, Loss: 0.10456492440658621\n",
    "Model and tokenizer saved to strategyrewrite_generate/fine_tuned_roberta_model_1e-7\n",
    "Epoch 1/3, Loss: 0.5568382432870567\n",
    "Epoch 2/3, Loss: 0.18018147862516343\n",
    "Epoch 3/3, Loss: 0.13946095401188358\n",
    "Model and tokenizer saved to strategymerge_generate/fine_tuned_roberta_model_1e-7\n",
    "Epoch 1/3, Loss: 0.49194131010522446\n",
    "Epoch 2/3, Loss: 0.16421591536452373\n",
    "Epoch 3/3, Loss: 0.12406963662050353\n",
    "Model and tokenizer saved to strategyrewrite_merge_generate/fine_tuned_roberta_model_1e-7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "05 11 24:\n",
    "\n",
    "model already trained\n",
    "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
    "  warnings.warn(\n",
    "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "  warnings.warn(\n",
    "1it [00:09,  9.24s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "200it [01:29,  2.23it/s]\n",
    "Epoch 1/3, Loss: 0.5377225166221615\n",
    "1it [00:08,  8.33s/it]\n",
    "Average Score on Test Set: 0.04804430112941191\n",
    "200it [01:28,  2.26it/s]\n",
    "Epoch 2/3, Loss: 0.07451270430552541\n",
    "1it [00:08,  8.56s/it]\n",
    "Average Score on Test Set: 0.015008262076880783\n",
    "200it [01:28,  2.25it/s]\n",
    "Epoch 3/3, Loss: 0.030714410086657155\n",
    "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
    "  warnings.warn(\n",
    "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "  warnings.warn(\n",
    "1it [00:08,  8.74s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "200it [01:28,  2.26it/s]\n",
    "Epoch 1/3, Loss: 0.4872837724350393\n",
    "1it [00:08,  8.29s/it]\n",
    "Average Score on Test Set: 0.04224143780767917\n",
    "200it [01:29,  2.23it/s]\n",
    "Epoch 2/3, Loss: 0.180051958358963\n",
    "1it [00:08,  8.73s/it]\n",
    "Average Score on Test Set: 0.016122875302680768\n",
    "200it [01:30,  2.20it/s]\n",
    "Epoch 3/3, Loss: 0.12139478604512988\n",
    "1it [00:08,  8.97s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "200it [01:27,  2.29it/s]\n",
    "Epoch 1/3, Loss: 0.28334643679772853\n",
    "1it [00:08,  8.38s/it]\n",
    "Average Score on Test Set: 0.0018397398947854527\n",
    "200it [01:26,  2.30it/s]\n",
    "Epoch 2/3, Loss: 0.003034894197480753\n",
    "1it [00:08,  8.59s/it]\n",
    "Average Score on Test Set: 0.00017533948106574827\n",
    "200it [01:26,  2.31it/s]\n",
    "Epoch 3/3, Loss: 0.0001495654441168881\n",
    "1it [00:08,  8.88s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "400it [02:48,  2.38it/s]\n",
    "Epoch 1/3, Loss: 0.37588352467573716\n",
    "1it [00:08,  8.19s/it]\n",
    "Average Score on Test Set: 0.09794454607181251\n",
    "400it [02:48,  2.38it/s]\n",
    "Epoch 2/3, Loss: 0.09809963616025925\n",
    "1it [00:08,  8.28s/it]\n",
    "Average Score on Test Set: 0.026430218790192157\n",
    "400it [02:50,  2.35it/s]\n",
    "Epoch 3/3, Loss: 0.07784116613755032\n",
    "1it [00:08,  8.43s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "400it [02:49,  2.36it/s]\n",
    "Epoch 1/3, Loss: 0.2515395031653316\n",
    "1it [00:08,  8.55s/it]\n",
    "Average Score on Test Set: 0.019773992223781532\n",
    "400it [02:48,  2.37it/s]\n",
    "Epoch 2/3, Loss: 0.02078622024422657\n",
    "1it [00:08,  8.34s/it]\n",
    "Average Score on Test Set: 0.002458631857880391\n",
    "400it [02:46,  2.41it/s]\n",
    "Epoch 3/3, Loss: 0.01462013486539945\n",
    "1it [00:08,  8.48s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "400it [02:50,  2.35it/s]\n",
    "Epoch 1/3, Loss: 0.32009216177008054\n",
    "1it [00:08,  8.30s/it]\n",
    "Average Score on Test Set: 0.0009409943097853102\n",
    "400it [02:50,  2.35it/s]\n",
    "Epoch 2/3, Loss: 0.09120187416174304\n",
    "1it [00:08,  8.51s/it]\n",
    "Average Score on Test Set: 0.0028826505088363776\n",
    "400it [02:48,  2.37it/s]\n",
    "Epoch 3/3, Loss: 0.02774700453112018\n",
    "1it [00:08,  8.22s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "501it [03:34,  2.69s/it]\n",
    "Average Score on Test Set: 0.007754787359153852\n",
    "600it [04:13,  2.37it/s]\n",
    "Epoch 1/3, Loss: 0.24971465460407974\n",
    "1it [00:08,  8.45s/it]\n",
    "Average Score on Test Set: 0.004358874907484278\n",
    "501it [03:53,  3.02s/it]\n",
    "Average Score on Test Set: 0.0038914726627990604\n",
    "600it [04:35,  2.18it/s]\n",
    "Epoch 2/3, Loss: 0.06317964661491715\n",
    "1it [00:09,  9.33s/it]\n",
    "Average Score on Test Set: 0.005353602179093286\n",
    "501it [03:48,  3.00s/it]\n",
    "Average Score on Test Set: 0.008322484973468817\n",
    "600it [04:29,  2.23it/s]\n",
    "Epoch 3/3, Loss: 0.03903801586528668\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######_VARIANT061124_WITHPROB_TEMP2       ------> TEMP IS NOT REALLY 2 IT IS 1\n",
    "\n",
    "model already trained\n",
    "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "  warnings.warn(\n",
    "['rewrite']\n",
    "1it [00:09,  9.32s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "200it [01:34,  2.12it/s]\n",
    "Epoch 1/3, Loss: 0.631802698224783\n",
    "1it [00:08,  8.90s/it]\n",
    "Average Score on Test Set: 0.03377865324961021\n",
    "200it [01:31,  2.19it/s]\n",
    "Epoch 2/3, Loss: 0.5150543220341206\n",
    "1it [00:08,  8.88s/it]\n",
    "Average Score on Test Set: 0.006201645165274386\n",
    "200it [01:32,  2.16it/s]\n",
    "Epoch 3/3, Loss: 0.5109683550894261\n",
    "['merge']\n",
    "1it [00:09,  9.23s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "200it [01:34,  2.12it/s]\n",
    "Epoch 1/3, Loss: 0.6243726582825184\n",
    "1it [00:08,  8.74s/it]\n",
    "Average Score on Test Set: 0.0010155325659434312\n",
    "200it [01:34,  2.12it/s]\n",
    "Epoch 2/3, Loss: 0.5298463521897793\n",
    "1it [00:09,  9.46s/it]\n",
    "Average Score on Test Set: 0.011833054920716677\n",
    "200it [01:34,  2.11it/s]\n",
    "Epoch 3/3, Loss: 0.5230607627332211\n",
    "['generate']\n",
    "1it [00:08,  8.75s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "200it [01:30,  2.21it/s]\n",
    "Epoch 1/3, Loss: 0.5628629119694233\n",
    "1it [00:08,  8.79s/it]\n",
    "Average Score on Test Set: 0.0003837848060356919\n",
    "200it [01:31,  2.20it/s]\n",
    "Epoch 2/3, Loss: 0.5047738802433014\n",
    "1it [00:09,  9.22s/it]\n",
    "Average Score on Test Set: 0.00025973492425691803\n",
    "200it [01:31,  2.18it/s]\n",
    "Epoch 3/3, Loss: 0.504714647680521\n",
    "\n",
    "['rewrite', 'merge', 'generate']\n",
    "1it [00:10, 10.27s/it]\n",
    "Average Score on Test Set: 0.21149490237236024\n",
    "501it [03:50,  2.99s/it]\n",
    "Average Score on Test Set: 0.0009219523920910433\n",
    "600it [04:33,  2.20it/s]\n",
    "Epoch 1/3, Loss: 0.5571168273687362\n",
    "1it [00:08,  8.81s/it]\n",
    "Average Score on Test Set: 0.0002855340932001127\n",
    "501it [03:49,  3.00s/it]\n",
    "Average Score on Test Set: 0.0005405940045602619\n",
    "600it [04:31,  2.21it/s]\n",
    "Epoch 2/3, Loss: 0.511881186068058\n",
    "1it [00:08,  8.90s/it]\n",
    "Average Score on Test Set: 0.00012868218091171\n",
    "501it [03:48,  3.00s/it]\n",
    "Average Score on Test Set: 0.00011664028126688208\n",
    "600it [04:31,  2.21it/s]\n",
    "Epoch 3/3, Loss: 0.508213345358769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1594fb-6a81-4ea4-84a1-884a2348378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Ensure the model is on the correct device\n",
    "    scores = []\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probabilities = torch.sigmoid(logits).squeeze(-1)\n",
    "\n",
    "            # Append probabilities (scores) to list\n",
    "            scores.extend(probabilities.cpu().numpy())  # Move to CPU and convert to numpy for easy handling\n",
    "\n",
    "    # Calculate the average score across all test elements\n",
    "    average_score = sum(scores) / len(scores) if scores else 0\n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e913da5d-3305-4476-9075-7d44c1913e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2422eee-d0f0-48ae-bc2e-6d3f1ea31762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/bussotti/.conda/envs/ForLLMLora/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rewrite', 'merge', 'generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:10, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score on Test Set: 0.21149490237236024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [03:50,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score on Test Set: 0.0009219523920910433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [04:33,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.5571168273687362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:08,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score on Test Set: 0.0002855340932001127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [03:49,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score on Test Set: 0.0005405940045602619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [04:31,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.511881186068058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:08,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score on Test Set: 0.00012868218091171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [03:48,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score on Test Set: 0.00011664028126688208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [04:31,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.508213345358769\n"
     ]
    }
   ],
   "source": [
    "###########FROM GPT_ VARIANT051124\n",
    "\n",
    "\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "for strategy in to_implement:\n",
    "    if len(strategy)<3:\n",
    "        continue\n",
    "    name_folder='strategy'+'_'.join(strategy)\n",
    "    save_directory = name_folder+\"/fine_tuned_roberta_VARIANT061124_WITHPROB_TEMP2\"\n",
    "    if os.path.exists(save_directory) or len(strategy)==0:\n",
    "        print('model already trained')\n",
    "        continue\n",
    "    if not os.path.exists(name_folder):\n",
    "        os.mkdir(name_folder)\n",
    "    train_texts=[]\n",
    "    train_labels=[]\n",
    "    for key in strategy:\n",
    "        train_texts+=obj[key]\n",
    "        train_labels+=[1 for x in obj[key]]\n",
    "    train_texts+=txt_emails_to_use[:len(train_texts)]\n",
    "    train_labels+=[0 for x in train_labels]\n",
    "\n",
    "    model = RobertaClassifier.from_pretrained(\"SuperAnnotate/roberta-large-llm-content-detector\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"SuperAnnotate/roberta-large-llm-content-detector\")\n",
    "\n",
    "    model.roberta.config.hidden_dropout_prob = 0.2\n",
    "    \n",
    "    # Data preparation\n",
    "    test_dataset = TextDataset(txt_emails_to_test, [0 for x in txt_emails_to_test], tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    \n",
    "    # Data preparation\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Fine-tuning setup\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01, betas=(0.9, 0.98))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Scheduler setup\n",
    "    num_epochs = 3\n",
    "    accumulation_steps = 8  # Adjust this based on desired effective batch size\n",
    "    num_training_steps = num_epochs * len(train_loader) // accumulation_steps\n",
    "    warmup_steps = num_training_steps // 10  # 10% of total steps as warm-up\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "    evaluate_every=500\n",
    "    # Training loop with gradient accumulation and scheduler\n",
    "    print(strategy)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for i, batch in tqdm(enumerate(train_loader)):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "    \n",
    "            # Forward pass\n",
    "          \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)[1]  \n",
    "\n",
    "\n",
    "            ######WITHOUT PROB\n",
    "            \n",
    "            #######WITH PROB\n",
    "            temperature = 1.0  # Adjust as needed; higher values give smoother probabilities\n",
    "            probabilities = torch.sigmoid(logits.squeeze(-1) / temperature)\n",
    "\n",
    "            # Loss calculation\n",
    "            ######WITHOUT PROB\n",
    "            #loss = F.binary_cross_entropy_with_logits(logits.squeeze(-1), labels)\n",
    "            #######WITH PROB\n",
    "            loss = F.binary_cross_entropy_with_logits(probabilities, labels)\n",
    "\n",
    "            \n",
    "            loss = loss / accumulation_steps  # Scale loss for gradient accumulation\n",
    "            loss.backward()\n",
    "    \n",
    "            # Accumulate gradients and update every `accumulation_steps`\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "    \n",
    "            total_loss += loss.item() * accumulation_steps  # Accumulate actual loss for logging\n",
    "            if i%evaluate_every==0:\n",
    "                # Assuming `test_loader` is defined\n",
    "                average_score = evaluate_model(model, test_loader)\n",
    "                print(\"Average Score on Test Set:\", average_score)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "    \n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51123a2d-dc64-47f0-bb3e-463a869a12f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rewrite', 'merge', 'generate']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "477a0f65-cb91-4325-b9b8-4a2f22a9fe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['rewrite'],\n",
       " ['merge'],\n",
       " ['generate'],\n",
       " ['rewrite', 'merge'],\n",
       " ['rewrite', 'generate'],\n",
       " ['merge', 'generate'],\n",
       " ['rewrite', 'merge', 'generate']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479c052-e5ce-444e-b169-a2785b8ce7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForLLMLora August",
   "language": "python",
   "name": "forllmlora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
